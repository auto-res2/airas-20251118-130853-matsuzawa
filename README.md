# Elastic Hierarchical Agreement–Curvature Budgeting for Trust-Region Controlled Fine-Tuning
> ⚠️ **NOTE:** This research is an automatic research using AIRAS.
## Abstract
We address the problem of allocating a fixed optimisation budget when fully fine-tuning transformer language models under tight compute or energy limits. Existing agreement-based controllers increase the learning rate of layers whose training and evaluation gradients align, but they operate at layer granularity and irrevocably freeze layers once alignment turns negative, leaving performance on the table. We introduce HACBO, a Hierarchical Agreement–Curvature Budgeted Optimiser that retains a constant global trust-region radius, first distributing it across layers in proportion to the positive part of train–dev gradient cosine similarity and then, inside each layer, across sub-modules according to inverse curvature estimated from exponential moving averages of RMS gradient norms. Layers with persistently negative agreement are moved to an int8 probation state with a 0.1× update scale, keeping them inexpensive yet able to recover. A rolling micro-dev buffer regularly replaces solved validation items with current failures so that agreement remains informative throughout training. We instantiate HACBO for few-epoch supervised fine-tuning of the 0.6 B-parameter Qwen3 model on GSM8K and compare it against the strong baseline BLAC. Although both single-seed runs in the present logs diverge and achieve 0 % exact-match accuracy, the detailed configurations, energy traces, and controller statistics highlight stability levers—most notably the probation scale γ and the base learning rate—that will inform forthcoming multi-seed studies. HACBO is scale-free, introduces only six hyper-parameters with robust defaults, and provides a principled path toward elastic, evaluation-aware budget allocation.

- [Research history](https://github.com/auto-res2/airas-20251118-130853-matsuzawa/blob/main/.research/research_history.json)
- [GitHub Pages](https://auto-res2.github.io/airas-20251118-130853-matsuzawa/branches/main/index.html)