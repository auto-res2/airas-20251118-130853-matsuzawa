Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 172ms
Installed 98 packages in 5.24s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-ml-py==13.580.82
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pynvml==13.0.1
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py:18: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 185238.57 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 211900.07 examples/s]
tokenise-train (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]tokenise-train (num_proc=8):   2%|▏         | 116/7473 [00:00<01:00, 122.48 examples/s]tokenise-train (num_proc=8):   5%|▍         | 367/7473 [00:01<00:17, 407.87 examples/s]tokenise-train (num_proc=8):  23%|██▎       | 1691/7473 [00:01<00:02, 2283.81 examples/s]tokenise-train (num_proc=8):  39%|███▉      | 2922/7473 [00:01<00:01, 3924.75 examples/s]tokenise-train (num_proc=8):  55%|█████▌    | 4111/7473 [00:01<00:00, 5333.23 examples/s]tokenise-train (num_proc=8):  71%|███████   | 5299/7473 [00:01<00:00, 6725.35 examples/s]tokenise-train (num_proc=8):  85%|████████▌ | 6378/7473 [00:01<00:00, 7525.66 examples/s]tokenise-train (num_proc=8): 100%|██████████| 7473/7473 [00:01<00:00, 6406.92 examples/s]tokenise-train (num_proc=8): 100%|██████████| 7473/7473 [00:02<00:00, 3448.14 examples/s]
tokenise-dev (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]tokenise-dev (num_proc=8):   9%|▉         | 120/1319 [00:01<00:10, 116.69 examples/s]tokenise-dev (num_proc=8):  34%|███▍      | 448/1319 [00:01<00:01, 498.41 examples/s]tokenise-dev (num_proc=8):  50%|█████     | 660/1319 [00:01<00:00, 723.60 examples/s]tokenise-dev (num_proc=8):  72%|███████▏  | 944/1319 [00:01<00:00, 1000.70 examples/s]tokenise-dev (num_proc=8):  87%|████████▋ | 1154/1319 [00:01<00:00, 939.89 examples/s]tokenise-dev (num_proc=8): 100%|██████████| 1319/1319 [00:02<00:00, 624.01 examples/s]
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 91ms
Installed 98 packages in 5.03s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-ml-py==13.580.82
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pynvml==13.0.1
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py:18: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 178297.76 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 203250.93 examples/s]
tokenise-train (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]tokenise-train (num_proc=8):   2%|▏         | 118/7473 [00:01<01:06, 111.06 examples/s]tokenise-train (num_proc=8):  12%|█▏        | 893/7473 [00:01<00:06, 1004.11 examples/s]tokenise-train (num_proc=8):  25%|██▌       | 1882/7473 [00:01<00:02, 2224.45 examples/s]tokenise-train (num_proc=8):  39%|███▊      | 2885/7473 [00:01<00:01, 3492.46 examples/s]tokenise-train (num_proc=8):  55%|█████▌    | 4122/7473 [00:01<00:00, 5119.55 examples/s]tokenise-train (num_proc=8):  71%|███████   | 5295/7473 [00:01<00:00, 6518.95 examples/s]tokenise-train (num_proc=8):  88%|████████▊ | 6540/7473 [00:01<00:00, 7598.56 examples/s]tokenise-train (num_proc=8): 100%|██████████| 7473/7473 [00:02<00:00, 3217.96 examples/s]
tokenise-dev (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]tokenise-dev (num_proc=8):   9%|▉         | 116/1319 [00:01<00:12, 96.44 examples/s]tokenise-dev (num_proc=8):  34%|███▍      | 449/1319 [00:01<00:02, 428.98 examples/s]tokenise-dev (num_proc=8):  50%|█████     | 660/1319 [00:01<00:01, 637.76 examples/s]tokenise-dev (num_proc=8):  72%|███████▏  | 946/1319 [00:01<00:00, 927.52 examples/s]tokenise-dev (num_proc=8):  88%|████████▊ | 1155/1319 [00:01<00:00, 1095.80 examples/s]tokenise-dev (num_proc=8): 100%|██████████| 1319/1319 [00:02<00:00, 631.56 examples/s] 
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 561ms
Installed 98 packages in 5.22s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-ml-py==13.580.82
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pynvml==13.0.1
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py:18: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[I 2025-11-18 15:37:12,138] A new study created in memory with name: no-name-83abb09e-32d5-4524-9c1b-11e4a8464cf1
[W 2025-11-18 15:37:12,138] Trial 0 failed with parameters: {} because of the following error: AttributeError("type object 'OmegaConf' has no attribute 'deepcopy'").
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 478, in _objective
    cfg = OmegaConf.deepcopy(base_cfg)
          ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'
[W 2025-11-18 15:37:12,139] Trial 0 failed with value None.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 553, in <module>
    study.optimize(_optuna_objective(cfg), n_trials=cfg.optuna.n_trials)
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 478, in _objective
    cfg = OmegaConf.deepcopy(base_cfg)
          ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'
Error executing job with overrides: ['run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py", line 58, in main
    subprocess.run(cmd, check=True, env=env)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/t-80-8-b-03/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'mode=full', 'results_dir=.research/iteration1']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 108 packages in 514ms
Installed 98 packages in 7.23s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-ml-py==13.580.82
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pynvml==13.0.1
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py:18: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[I 2025-11-19 00:39:48,941] A new study created in memory with name: no-name-2980c58d-0c8d-46a3-bebe-d72e506f7b05
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 107601.27 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 90432.31 examples/s]
tokenise-train (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]tokenise-train (num_proc=8):   2%|▏         | 131/7473 [00:01<00:59, 124.35 examples/s]tokenise-train (num_proc=8):   6%|▌         | 413/7473 [00:01<00:15, 446.16 examples/s]tokenise-train (num_proc=8):  25%|██▌       | 1904/7473 [00:01<00:02, 2537.07 examples/s]tokenise-train (num_proc=8):  42%|████▏     | 3169/7473 [00:01<00:01, 4269.02 examples/s]tokenise-train (num_proc=8):  62%|██████▏   | 4606/7473 [00:01<00:00, 5865.98 examples/s]tokenise-train (num_proc=8):  76%|███████▌  | 5689/7473 [00:01<00:00, 6887.16 examples/s]tokenise-train (num_proc=8):  91%|█████████▏| 6827/7473 [00:01<00:00, 7676.94 examples/s]tokenise-train (num_proc=8): 100%|██████████| 7473/7473 [00:02<00:00, 3526.59 examples/s]
tokenise-dev (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]tokenise-dev (num_proc=8):  10%|█         | 134/1319 [00:00<00:08, 138.11 examples/s]tokenise-dev (num_proc=8):  25%|██▌       | 330/1319 [00:01<00:02, 365.88 examples/s]tokenise-dev (num_proc=8):  48%|████▊     | 631/1319 [00:01<00:00, 729.79 examples/s]tokenise-dev (num_proc=8):  60%|██████    | 797/1319 [00:01<00:00, 881.27 examples/s]tokenise-dev (num_proc=8):  75%|███████▌  | 990/1319 [00:01<00:00, 1057.81 examples/s]tokenise-dev (num_proc=8):  98%|█████████▊| 1290/1319 [00:01<00:00, 1365.96 examples/s]tokenise-dev (num_proc=8): 100%|██████████| 1319/1319 [00:01<00:00, 716.88 examples/s] 
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[W 2025-11-19 00:40:43,180] Trial 0 failed with parameters: {'base_learning_rate': 8.610298152353233e-05, 'K': 40, 'rho': 0.7337336860113749, 'U': 150} because of the following error: RuntimeError('element 0 of tensors does not require grad and does not have a grad_fn').
Traceback (most recent call last):
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 504, in _objective
    acc = _single_run(cfg)
          ^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 409, in _single_run
    controller.on_update_end(train_grads)
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 162, in on_update_end
    self._agreement_measure(train_grads)
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 138, in _agreement_measure
    dev_loss.backward()
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[W 2025-11-19 00:40:43,183] Trial 0 failed with value None.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 554, in <module>
    study.optimize(_optuna_objective(cfg), n_trials=cfg.optuna.n_trials)
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 504, in _objective
    acc = _single_run(cfg)
          ^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 409, in _single_run
    controller.on_update_end(train_grads)
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 162, in on_update_end
    self._agreement_measure(train_grads)
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 138, in _agreement_measure
    dev_loss.backward()
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
Error executing job with overrides: ['run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py", line 58, in main
    subprocess.run(cmd, check=True, env=env)
  File "/mnt/home/toma/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/mnt/home/toma/KRK-039/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'mode=full', 'results_dir=.research/iteration1']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 96ms
Installed 98 packages in 4.63s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.2
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.11.12
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-ml-py==13.580.82
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.6.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pynvml==13.0.1
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.45.0
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.1
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.23.0
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py:18: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[I 2025-11-18 15:43:58,767] A new study created in memory with name: no-name-2d59e1bb-a027-47b1-b8a1-75a4c18d5368
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 161922.75 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 157332.62 examples/s]
tokenise-train (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]tokenise-train (num_proc=8):   1%|          | 67/7473 [00:01<02:24, 51.23 examples/s]tokenise-train (num_proc=8):   5%|▌         | 396/7473 [00:01<00:20, 349.99 examples/s]tokenise-train (num_proc=8):  13%|█▎        | 965/7473 [00:01<00:07, 907.10 examples/s]tokenise-train (num_proc=8):  29%|██▉       | 2157/7473 [00:01<00:02, 2368.23 examples/s]tokenise-train (num_proc=8):  37%|███▋      | 2743/7473 [00:01<00:01, 2882.14 examples/s]tokenise-train (num_proc=8):  45%|████▍     | 3345/7473 [00:01<00:01, 3436.13 examples/s]tokenise-train (num_proc=8):  53%|█████▎    | 3938/7473 [00:02<00:00, 3949.42 examples/s]tokenise-train (num_proc=8):  61%|██████    | 4528/7473 [00:02<00:00, 4324.30 examples/s]tokenise-train (num_proc=8):  69%|██████▉   | 5145/7473 [00:02<00:00, 4770.71 examples/s]tokenise-train (num_proc=8):  78%|███████▊  | 5834/7473 [00:02<00:00, 5304.81 examples/s]tokenise-train (num_proc=8):  86%|████████▋ | 6464/7473 [00:02<00:00, 5499.49 examples/s]tokenise-train (num_proc=8):  95%|█████████▍| 7087/7473 [00:02<00:00, 5000.31 examples/s]tokenise-train (num_proc=8): 100%|██████████| 7473/7473 [00:03<00:00, 2318.77 examples/s]
tokenise-dev (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]tokenise-dev (num_proc=8):   5%|▌         | 72/1319 [00:01<00:21, 56.70 examples/s]tokenise-dev (num_proc=8):  24%|██▎       | 313/1319 [00:01<00:03, 280.20 examples/s]tokenise-dev (num_proc=8):  36%|███▌      | 478/1319 [00:01<00:02, 408.00 examples/s]tokenise-dev (num_proc=8):  50%|█████     | 660/1319 [00:01<00:01, 593.56 examples/s]tokenise-dev (num_proc=8):  61%|██████▏   | 811/1319 [00:01<00:00, 694.20 examples/s]tokenise-dev (num_proc=8):  74%|███████▍  | 979/1319 [00:01<00:00, 802.90 examples/s]tokenise-dev (num_proc=8):  87%|████████▋ | 1142/1319 [00:02<00:00, 866.38 examples/s]tokenise-dev (num_proc=8):  99%|█████████▉| 1307/1319 [00:02<00:00, 960.70 examples/s]tokenise-dev (num_proc=8): 100%|██████████| 1319/1319 [00:02<00:00, 521.62 examples/s]
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[W 2025-11-18 15:44:40,643] Trial 0 failed with parameters: {'base_learning_rate': 0.0001639287271323173, 'K': 20, 'rho': 0.7371291045595653, 'U': 100} because of the following error: RuntimeError('element 0 of tensors does not require grad and does not have a grad_fn').
Traceback (most recent call last):
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 502, in _objective
    acc = _single_run(cfg)
          ^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 407, in _single_run
    controller.on_update_end(train_grads)
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 161, in on_update_end
    self._agreement_measure(train_grads)
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 137, in _agreement_measure
    dev_loss.backward()
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[W 2025-11-18 15:44:40,646] Trial 0 failed with value None.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 552, in <module>
    study.optimize(_optuna_objective(cfg), n_trials=cfg.optuna.n_trials)
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 502, in _objective
    acc = _single_run(cfg)
          ^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 407, in _single_run
    controller.on_update_end(train_grads)
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 161, in on_update_end
    self._agreement_measure(train_grads)
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/train.py", line 137, in _agreement_measure
    dev_loss.backward()
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
Error executing job with overrides: ['run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/src/main.py", line 58, in main
    subprocess.run(cmd, check=True, env=env)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/pt80-1-a-29/_work/airas-20251118-130853-matsuzawa/airas-20251118-130853-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-iter1-Qwen3-0.6B-gsm8k', 'mode=full', 'results_dir=.research/iteration1']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
