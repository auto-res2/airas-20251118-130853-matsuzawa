@misc{airas2025,
  author    = {Toma Tanaka and Takumi Matsuzawa and Yuki Yoshino and Ilya Horiguchi and Shiro Takagi and Ryutaro Yamauchi and Wataru Kumagai},
  title     = {{AIRAS}},
  year      = {2025},
  publisher = {GitHub},
  url       = {https://github.com/airas-org/airas}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{author-year-autolrs,
 title = {AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly}
}

@article{author-year-careful,
 title = {A Careful Examination of Large Language Model Performance on Grade School Arithmetic}
}

@article{author-year-evaluating,
 title = {Evaluating Quantized Large Language Models}
}

@article{author-year-lean,
 title = {Lean Workbook: A large-scale Lean problem set formalized from natural language math problems}
}

@article{author-year-lego,
 title = {LEGO-Prover: Neural Theorem Proving with Growing Libraries}
}

@article{author-year-llemma,
 title = {Llemma: An Open Language Model for Mathematics}
}

@article{author-year-lora,
 title = {QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models}
}

@article{author-year-mechanic,
 title = {Mechanic: A Learning Rate Tuner}
}

@article{author-year-metamath,
 title = {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}
}

@article{author-year-momo,
 title = {MoMo: Momentum Models for Adaptive Learning Rates}
}

@article{author-year-multirate,
 title = {Multirate Training of Neural Networks}
}

@article{author-year-openmathinstruct,
 title = {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}
}

@article{author-year-prodigy,
 title = {Prodigy: An Expeditiously Adaptive Parameter-Free Learner}
}

@article{author-year-qlora,
 title = {QLoRA: Efficient Finetuning of Quantized LLMs}
}

@article{author-year-quanta,
 title = {QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation}
}

@article{author-year-reverse,
 title = {Reverse engineering learned optimizers reveals known and novel mechanisms}
}

@article{author-year-training,
 title = {Training Chain-of-Thought via Latent-Variable Inference}
}

@article{author-year-where,
 title = {Where Do Large Learning Rates Lead Us?}
}